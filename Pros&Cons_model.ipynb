{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjUUZbVJCrIr"
      },
      "source": [
        "# **Auto-Review Analyzer: AI-Powered Pros & Cons Extraction & Summarization** #\n",
        "### *A scalable NLP system for automatically identifying, classifying, and summarizing positive and negative reviews using Transformer architectures and LoRA fine-tuning*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXjx2c1w7bDy"
      },
      "source": [
        "## 1. Project Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy-zxH6s4VpH"
      },
      "source": [
        "### 1.1 Objective & Scope\n",
        "This project delivers a scalable prototype for automatically identifying **pros** and **cons** from user-generated text, such as reviews, comments, and feedback, and summarizing key insights concisely. While many platforms (e.g., Amazon, Mercado Libre) offer aggregated feedback summaries, explicitly distinguishing between pros and cons provides clearer, more actionable information for decision-making and user satisfaction.\n",
        "\n",
        "The focus of this project is on building a adaptable foundation for future development rather than a finalized production system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFaXChYg4dmM"
      },
      "source": [
        "### 1.2 Solution & Technical Approach\n",
        "I developed a **two-layer Transformer encoder classifier**, optimized for both accuracy and efficiency. The model was trained on a curated dataset of **~1.7 million Glassdoor job reviews**, with pros and cons carefully extracted and structured for supervised learning. To maximize performance within limited resources, we fine-tuned the model using **LoRA (Low-Rank Adaptation)**, a parameter-efficient technique that reduces training time and computational cost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Og80QNFQ4fN9"
      },
      "source": [
        "### 1.3 Development Context & Constraints\n",
        "Development was conducted under tight practical constraints, including limited Colab GPU availability, RAM, GPU memory, and storage. Despite these challenges, the resulting model performs reliably in tests and produces consistent, interpretable outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cuyy-FP8OL2G"
      },
      "source": [
        "## 2. Environment setup\n",
        "\n",
        "In this section we prepare our environment to prevent compatibility issues. We begin by clearing any conflicts, then install pinned, compatible versions of all required libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7H5q6Nl4_9g"
      },
      "source": [
        "### 2.1 Dependency Pinning & Version Control\n",
        "To ensure a stable training environment and prevent compatibility issues, the first step is to remove any preinstalled versions of PyTorch, Transformers, NumPy, and related libraries in the Colab environment. Mixing library versions across sessions can introduce runtime errors, CUDA conflicts, or subtle changes in model behavior.\n",
        "\n",
        "After cleanup, we reinstall pinned, mutually compatible versions of the core dependencies (PyTorch, Transformers, PEFT, Accelerate, and others). This approach ensures reproducibility and guarantees that all components function correctly together, particularly during LoRA fine-tuning and later deployment of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UReJ_1LxGyA6",
        "outputId": "2f16dae0-41f3-4a18-e52a-051255779b68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torchdata as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "timm 1.0.22 requires torchvision, which is not installed.\n",
            "torchtune 0.6.1 requires torchdata==0.11.0, which is not installed.\n",
            "fastai 2.8.6 requires torchvision>=0.11, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Environment cleanup (to avoid version conflicts)\n",
        "!pip uninstall -y -q \\\n",
        "    torch \\\n",
        "    torchvision \\\n",
        "    torchaudio \\\n",
        "    torchtext \\\n",
        "    torchdata \\\n",
        "    numpy \\\n",
        "    transformers \\\n",
        "    peft\n",
        "\n",
        "# Install pinned, compatible versions\n",
        "!pip install -q \\\n",
        "    torch==2.3.0 \\\n",
        "    torchtext==0.18.0 \\\n",
        "    numpy==2.0.0 \\\n",
        "    transformers==4.41.0 \\\n",
        "    peft==0.11.0 \\\n",
        "    sentencepiece==0.2.0 \\\n",
        "    safetensors==0.4.3 \\\n",
        "    accelerate==0.29.3 \\\n",
        "    gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoTIbomHOtlu"
      },
      "source": [
        "Once the libraries are installed, the Python environment must be restarted so\n",
        "that the new versions are properly loaded. Here we trigger a clean restart using `os._exit(0)`, which terminates the current process. And we manually reload the session with the freshly installed packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9kcxLih7dkn"
      },
      "outputs": [],
      "source": [
        "# Restart runtime after installation\n",
        "import os\n",
        "os._exit(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2EOaE67aptt"
      },
      "source": [
        "### 2.2 Library Installation & GPU/CPU Setup\n",
        "\n",
        "With the environment ready, we move on to importing the main libraries for the project. These cover data handling (NumPy and pandas), plotting (Matplotlib), and general utilities like tqdm and scikit-learn. We use PyTorch and TorchText for training and text preprocessing, and Transformers together with PEFT for LoRA fine-tuning. We also bring in NLTK, Gradio, and a few Colab/Kaggle helpers when needed. At the end, we print some environment details to check GPU support and confirm the library versions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWtQc74Nl8Pf",
        "outputId": "4d6523a4-2b1e-452a-bc66-89e433d77a35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch: 2.3.0+cu121\n",
            "CUDA Available: True\n",
            "GPU: Tesla T4\n",
            "CUDA Version: 12.1\n"
          ]
        }
      ],
      "source": [
        "# Standard libraries\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Numericals & data handling\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Utilities\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# PyTorch & TorchText utilities\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "torch.set_num_threads(1)\n",
        "\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torchtext.vocab import (\n",
        "    build_vocab_from_iterator,\n",
        "    GloVe,\n",
        "    Vectors,\n",
        "    vocab as create_vocab\n",
        ")\n",
        "\n",
        "# Transformers & PEFT\n",
        "from transformers import (\n",
        "    pipeline,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    BitsAndBytesConfig,\n",
        "    logging\n",
        ")\n",
        "logging.set_verbosity_error() # Suppress Transformers warnings\n",
        "\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# NLP utilities\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "# UI & External services\n",
        "import gdown\n",
        "import kagglehub\n",
        "import gradio as gr\n",
        "from google.colab import files\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# Environment info\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igFPP7xu544i"
      },
      "source": [
        "## 3. Data Preprocessing\n",
        "\n",
        "This section covers the complete data preparation pipeline. We begin by downloading the training dataset and implementing a custom Dataset class for preprocessing. We then instantiate training, evaluation, and test data iterators. Following this, we load pre-trained GloVe embeddings to build the vocabulary and create a tokenization pipeline. A custom collation function is defined to form batches. Finally, we configure the data loaders to enable efficient batch processing during model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzJ0aNFPPVdl"
      },
      "source": [
        "### 3.1 Dataset Loading & Exploration\n",
        "\n",
        "First, we download the Glassdoor Job Reviews dataset from Kaggle using `kagglehub` and load it into a pandas DataFrame. The dataset includes user reviews with separate *pros* and *cons* fields, which will be used as training data for the classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvtwG2ygkHnb",
        "outputId": "d711b6c6-9939-4ae0-ba99-f9f29b4f13be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'glassdoor-job-reviews' dataset.\n"
          ]
        }
      ],
      "source": [
        "# Download Glassdoor Job Reviews dataset from Kaggle\n",
        "path_training_set = kagglehub.dataset_download(\"davidgauthier/glassdoor-job-reviews\")\n",
        "# Load dataset into a Pandas DataFrame\n",
        "gs_data = pd.read_csv(f\"{path_training_set}/glassdoor_reviews.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNCbILaAdQ3D"
      },
      "source": [
        "The Glassdoor dataset contains about 838k rows and 18 columns, including company information, job titles, ratings, and other metadata. For this project, the most important fields are **pros** and **cons**. However, because each review provides both a pros entry and a cons entry, we effectively have around 1.7 million text samples available for training, validation, and testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jr3_MYHacUGi",
        "outputId": "036753bd-3239-49f4-a378-8254c1baadd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 838566 entries, 0 to 838565\n",
            "Data columns (total 18 columns):\n",
            " #   Column               Non-Null Count   Dtype  \n",
            "---  ------               --------------   -----  \n",
            " 0   firm                 838566 non-null  object \n",
            " 1   date_review          838566 non-null  object \n",
            " 2   job_title            838566 non-null  object \n",
            " 3   current              838566 non-null  object \n",
            " 4   location             541223 non-null  object \n",
            " 5   overall_rating       838566 non-null  int64  \n",
            " 6   work_life_balance    688672 non-null  float64\n",
            " 7   culture_values       647193 non-null  float64\n",
            " 8   diversity_inclusion  136066 non-null  float64\n",
            " 9   career_opp           691065 non-null  float64\n",
            " 10  comp_benefits        688484 non-null  float64\n",
            " 11  senior_mgmt          682690 non-null  float64\n",
            " 12  recommend            838566 non-null  object \n",
            " 13  ceo_approv           838566 non-null  object \n",
            " 14  outlook              838566 non-null  object \n",
            " 15  headline             835976 non-null  object \n",
            " 16  pros                 838564 non-null  object \n",
            " 17  cons                 838553 non-null  object \n",
            "dtypes: float64(6), int64(1), object(11)\n",
            "memory usage: 115.2+ MB\n"
          ]
        }
      ],
      "source": [
        "gs_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_DUzlQLdzar"
      },
      "source": [
        "We display three random reviews from the dataset to quickly inspect the content.\n",
        "This lets us confirm that the `pros` and `cons` text fields are correctly loaded and formatted before moving on to preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "TtX2ZsVLcWdh",
        "outputId": "a0c4fcce-9289-4569-ca0b-d9242f530557"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                          firm date_review  \\\n",
              "408130       Iron-Mountain-Inc  2021-01-28   \n",
              "34758                    Apple  2016-03-30   \n",
              "495589  Marriott-International  2017-12-06   \n",
              "\n",
              "                                              job_title  \\\n",
              "408130                              Warehouse Associate   \n",
              "34758    IT Support Technician and Help Desk Specialist   \n",
              "495589                               Anonymous Employee   \n",
              "\n",
              "                                     current          location  \\\n",
              "408130     Former Employee, less than 1 year     Livermore, CA   \n",
              "34758   Current Employee, more than 10 years  Sherman Oaks, CA   \n",
              "495589                      Current Employee               NaN   \n",
              "\n",
              "        overall_rating  work_life_balance  culture_values  \\\n",
              "408130               1                1.0             1.0   \n",
              "34758                5                4.0             4.0   \n",
              "495589               4                3.0             4.0   \n",
              "\n",
              "        diversity_inclusion  career_opp  comp_benefits  senior_mgmt recommend  \\\n",
              "408130                  2.0         1.0            1.0          1.0         x   \n",
              "34758                   NaN         4.0            4.0          4.0         v   \n",
              "495589                  NaN         2.0            4.0          2.0         v   \n",
              "\n",
              "       ceo_approv outlook                    headline  \\\n",
              "408130          x       r     This place is terrible!   \n",
              "34758           o       v  Mac Genius or Home Advisor   \n",
              "495589          r       o                Good company   \n",
              "\n",
              "                                                     pros  \\\n",
              "408130                           None. There are no pros.   \n",
              "34758   A love of helping customers get the best under...   \n",
              "495589               Great benefits, great pay, free food   \n",
              "\n",
              "                                                     cons  \n",
              "408130  Everything! Low wages. Difficult co-workers. N...  \n",
              "34758   Knowing that I won't be able to support those ...  \n",
              "495589  Irregular hours, early mornings, no upward mob...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b7e8dedb-874e-4ffc-ad74-dfcf64817998\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>firm</th>\n",
              "      <th>date_review</th>\n",
              "      <th>job_title</th>\n",
              "      <th>current</th>\n",
              "      <th>location</th>\n",
              "      <th>overall_rating</th>\n",
              "      <th>work_life_balance</th>\n",
              "      <th>culture_values</th>\n",
              "      <th>diversity_inclusion</th>\n",
              "      <th>career_opp</th>\n",
              "      <th>comp_benefits</th>\n",
              "      <th>senior_mgmt</th>\n",
              "      <th>recommend</th>\n",
              "      <th>ceo_approv</th>\n",
              "      <th>outlook</th>\n",
              "      <th>headline</th>\n",
              "      <th>pros</th>\n",
              "      <th>cons</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>408130</th>\n",
              "      <td>Iron-Mountain-Inc</td>\n",
              "      <td>2021-01-28</td>\n",
              "      <td>Warehouse Associate</td>\n",
              "      <td>Former Employee, less than 1 year</td>\n",
              "      <td>Livermore, CA</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>x</td>\n",
              "      <td>x</td>\n",
              "      <td>r</td>\n",
              "      <td>This place is terrible!</td>\n",
              "      <td>None. There are no pros.</td>\n",
              "      <td>Everything! Low wages. Difficult co-workers. N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34758</th>\n",
              "      <td>Apple</td>\n",
              "      <td>2016-03-30</td>\n",
              "      <td>IT Support Technician and Help Desk Specialist</td>\n",
              "      <td>Current Employee, more than 10 years</td>\n",
              "      <td>Sherman Oaks, CA</td>\n",
              "      <td>5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>v</td>\n",
              "      <td>o</td>\n",
              "      <td>v</td>\n",
              "      <td>Mac Genius or Home Advisor</td>\n",
              "      <td>A love of helping customers get the best under...</td>\n",
              "      <td>Knowing that I won't be able to support those ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495589</th>\n",
              "      <td>Marriott-International</td>\n",
              "      <td>2017-12-06</td>\n",
              "      <td>Anonymous Employee</td>\n",
              "      <td>Current Employee</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>v</td>\n",
              "      <td>r</td>\n",
              "      <td>o</td>\n",
              "      <td>Good company</td>\n",
              "      <td>Great benefits, great pay, free food</td>\n",
              "      <td>Irregular hours, early mornings, no upward mob...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b7e8dedb-874e-4ffc-ad74-dfcf64817998')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b7e8dedb-874e-4ffc-ad74-dfcf64817998 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b7e8dedb-874e-4ffc-ad74-dfcf64817998');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a393d608-bcf9-4a9f-87e3-65babd81c4c5\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a393d608-bcf9-4a9f-87e3-65babd81c4c5')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a393d608-bcf9-4a9f-87e3-65babd81c4c5 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "0"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "gs_data.sample(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq13WhPWefqf"
      },
      "source": [
        "### 3.2 Binary Dataset Preparation\n",
        "\n",
        "After loading and inspecting the dataset, this step prepares the reviews so they can be used for training. The `GlassdoorDataset` class extracts the `pros` and `cons` columns, assigns label 1 to pros and label 0 to cons, removes missing entries, and performs an 85/15 train–test split with a fixed seed for reproducibility. The result is a clean binary dataset where each sample is returned in the form `(label, text)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xxIc_FTMr0j-"
      },
      "outputs": [],
      "source": [
        "class GlassdoorDataset(Dataset):\n",
        "    def __init__(self, train=True, random_state=42):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            train (bool): If True, return training split; otherwise test split.\n",
        "            random_state (int): Seed for reproducible train/test split.\n",
        "        \"\"\"\n",
        "        # Create labeled Pros data (Class = 1)\n",
        "        pros = pd.DataFrame({\n",
        "            \"Text\": gs_data.iloc[:, -2],\n",
        "            \"Class\": np.ones(len(gs_data), dtype=int)\n",
        "        })\n",
        "        # Create labeled Cons data (Class = 0)\n",
        "        cons = pd.DataFrame({\n",
        "            \"Text\": gs_data.iloc[:, -1],\n",
        "            \"Class\": np.zeros(len(gs_data), dtype=int)\n",
        "        })\n",
        "        # Combine Pros and Cons, remove missing entries\n",
        "        gs_df = (\n",
        "            pd.concat([pros, cons], ignore_index=True)\n",
        "              .dropna()\n",
        "        )\n",
        "        # Train / Test split\n",
        "        train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "            gs_df[\"Text\"].values,\n",
        "            gs_df[\"Class\"].values,\n",
        "            test_size=0.15,\n",
        "            random_state=random_state,\n",
        "            shuffle=True\n",
        "        )\n",
        "        # Select split based on `train` flag\n",
        "        if train:\n",
        "            self.texts = train_texts\n",
        "            self.labels = train_labels\n",
        "        else:\n",
        "            self.texts = test_texts\n",
        "            self.labels = test_labels\n",
        "\n",
        "        self.length = len(self.texts)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return dataset size.\"\"\"\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Return one sample:\n",
        "            (label, text)\n",
        "        \"\"\"\n",
        "        return self.labels[idx], self.texts[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTy5Avk7fG4I"
      },
      "source": [
        "Here we define a simple dictionary that maps numeric labels to readable names. Label 0 corresponds to “Con” and label 1 corresponds to “Pro”, this makes results easier to interpret when visualizing predictions or building the user interface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "thKfZcn4uBD5"
      },
      "outputs": [],
      "source": [
        "labels = {0: \"Con\", 1: \"Pro\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPDh0dP3fOvQ"
      },
      "source": [
        "Next, we create the training and test iterators using the `GlassdoorDataset` class, and then inspect one example to verify the labels and text look correct. The printed output confirms that a sample labeled `1` corresponds to a “Pro” entry, which helps validate that the dataset was built properly before moving on to modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3wexipGuGOu",
        "outputId": "55b7f1ba-fd48-4ad3-f8f1-3e376c55566e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 1 (Pro)\n",
            "Text: Worked there for a couple of years, you're colleague's made it worth it.  Pay was low but your told the pay before you start. Just a typical call centre.  At least I wasn't cold calling.\n"
          ]
        }
      ],
      "source": [
        "# Instantiate train and test datasets\n",
        "train_iter = GlassdoorDataset(train=True)\n",
        "test_iter = GlassdoorDataset(train=False)\n",
        "\n",
        "# Inspect a sample\n",
        "train_label, train_text = train_iter[5]\n",
        "print(f\"Label: {train_label} ({labels[train_label]})\")\n",
        "print(f\"Text: {train_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqogSwuUfVgI"
      },
      "source": [
        "Moving on, we convert the training and test datasets into map-style datasets so they can be indexed efficiently and used with PyTorch data loaders. We then split the training set into two parts: 80% for training and 20% for validation. The validation split is used to track performance and tune hyperparameters, while the test set remains untouched until final evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-Wxhj6_nIpC3"
      },
      "outputs": [],
      "source": [
        "# Convert the training and testing iterators to map-style datasets.\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "\n",
        "# Split train dataset into training (80%) and validation (20%)\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, valid_dataset = random_split(train_dataset, [train_size, val_size])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf48-yeBPlwO"
      },
      "source": [
        "### 3.3 Vocabulary Building with GloVe Embeddings\n",
        "\n",
        "Now, we prepare the data so it can be passed to the model. We load pretrained GloVe embeddings (100-dimensional) and use them to build the project vocabulary. The vocabulary is constructed directly from the GloVe token index and includes special tokens for unknown words and padding. Any token not found in GloVe is mapped to `<unk>`. We also record useful metadata such as the padding index and the total vocabulary size, which will be needed later when defining the model and creating the data loaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbAm-qKZ9kbp",
        "outputId": "56706911-3458-4d72-b834-a32f561d25b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pad index: 1 \n",
            "Vocab size:  400001\n"
          ]
        }
      ],
      "source": [
        "# Load pretrained GloVe embeddings\n",
        "glove_embedding = GloVe(name=\"6B\", dim=100, cache=\".vector_cache\")\n",
        "\n",
        "# Build vocabulary from GloVe\n",
        "vocab = create_vocab(\n",
        "    glove_embedding.stoi,\n",
        "    min_freq=1,\n",
        "    specials=[\"<unk>\", \"<pad>\"],\n",
        "    special_first=True\n",
        ")\n",
        "# Set default token for out-of-vocabulary words\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "# Vocabulary metadata\n",
        "PAD_IDX = vocab[\"<pad>\"]\n",
        "vocab_size = len(vocab)\n",
        "print(\"Pad index:\", PAD_IDX, \"\\nVocab size: \", vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpN7z_gYidYx"
      },
      "source": [
        "### 3.4 Tokenization & Batch Collation Pipeline\n",
        "This step defines the text preprocessing pipeline. We use TorchText’s `basic_english` tokenizer to split each review into tokens, and then convert those tokens into integer indices using the previously built vocabulary. The `text_pipeline` function will be applied later inside the data loaders so raw text can be fed directly into the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "90oJiUSnGqnI"
      },
      "outputs": [],
      "source": [
        "# Tokenizer\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "def text_pipeline(text):\n",
        "    \"\"\"\n",
        "    Convert raw text into a sequence of vocabulary indices.\n",
        "\n",
        "    Process:\n",
        "    - Tokenize input text using a basic English tokenizer.\n",
        "    - Map tokens to integer indices using the vocabulary.\n",
        "    \"\"\"\n",
        "    return vocab(tokenizer(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SPlsFH3itA2"
      },
      "source": [
        "Here we select the computation device. If a GPU is available, the model and tensors will run on CUDA; otherwise, the code falls back to the CPU. This ensures the notebook works both locally and in environments without GPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gdW2YNuLwlkt"
      },
      "outputs": [],
      "source": [
        "# Select GPU if available, otherwise CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBcXsoJCjCxP"
      },
      "source": [
        "The `collate_batch` function takes a batch of label–text pairs from a DataLoader and converts them into tensors suitable for model training. It turns each text example into token indices, pads the sequences so they all share the same length, converts the labels into a tensor, and returns both so the model receives a uniform batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "52zh7Ou-woCh"
      },
      "outputs": [],
      "source": [
        "def collate_batch(batch):\n",
        "    \"\"\"\n",
        "    Prepare a batch for training.\n",
        "\n",
        "    For each (label, text) pair:\n",
        "    - Convert text into a tensor of token indices\n",
        "    - Pad all sequences in the batch to the same length\n",
        "    - Convert labels into a tensor\n",
        "\n",
        "    Returns:\n",
        "        labels (LongTensor): shape (batch_size,)\n",
        "        texts  (LongTensor): shape (batch_size, max_seq_len)\n",
        "    \"\"\"\n",
        "    label_list = []\n",
        "    text_list = []\n",
        "\n",
        "    for label, text in batch:\n",
        "        # Store label\n",
        "        label_list.append(label)\n",
        "\n",
        "        # Tokenize text and convert to tensor\n",
        "        text_list.append(torch.tensor(text_pipeline(text), dtype=torch.int64))\n",
        "\n",
        "    # Convert labels to tensor\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    # Pad text sequences\n",
        "    text_list = pad_sequence(text_list, batch_first=True)\n",
        "\n",
        "    return label_list, text_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2Q1fZGGk4Tw"
      },
      "source": [
        "### 3.5 DataLoaders Configuration & Parallelized Batching\n",
        "\n",
        "Finally, we create the DataLoader objects to organize training, validation, and test datasets into mini-batches of fixed size, shuffle the training data, and rely on the `collate_batch` function to pad and format each batch correctly. They also use multiple worker processes to load data in parallel and enable faster transfers to the GPU, helping the model train and evaluate efficiently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "aaqZD2GmJzFA"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_batch,\n",
        "    num_workers=min(4, os.cpu_count() - 1),  # More workers\n",
        "    pin_memory=True,  # Faster data transfer to GPU\n",
        "    persistent_workers=True\n",
        ")\n",
        "valid_dataloader = DataLoader(\n",
        "    valid_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_batch,\n",
        "    num_workers=2\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_batch,\n",
        "    num_workers=2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r5G5QfvRbql"
      },
      "source": [
        "Next, the code below scans every review in both the training and test sets to determine the maximum sequence length. This value is then used to configure the transformer model's input size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mwvax-SUMu-g",
        "outputId": "9118f535-9339-4081-eb7d-020ce6980e36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of longest review: 3379\n"
          ]
        }
      ],
      "source": [
        "max_len = 0\n",
        "\n",
        "# Iterate over training reviews\n",
        "for review in list(train_iter[:][1]):\n",
        "    review_len = len(text_pipeline(review))\n",
        "    if review_len > max_len:\n",
        "        max_len = review_len\n",
        "\n",
        "# Iterate over test reviews\n",
        "for review in list(test_iter[:][1]):\n",
        "    review_len = len(text_pipeline(review))\n",
        "    if review_len > max_len:\n",
        "        max_len = review_len\n",
        "\n",
        "# Set maximum sequence length\n",
        "MAX_LEN = max_len\n",
        "\n",
        "print(\"Length of longest review:\", max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhZeRv4tSYG1"
      },
      "source": [
        "## 4. Model Development & Architecture\n",
        "\n",
        "In this section we detail the construction of the core classifier. It begins by implementing a standard positional encoding class to provide the model with sequence order. Next, we define the full Transformer encoder architecture, built on top of frozen GloVe embeddings, which processes text through attention layers and uses masked mean pooling. Finally, we cover the utility functions for making single predictions and evaluating the model's accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzNbLO4F9XcB"
      },
      "source": [
        "### 4.1 Positional Encoding Implementation\n",
        "\n",
        "Firstly, we create a class that implements sinusoidal positional encoding for Transformer models, following the original “Attention Is All You Need” paper. It precomputes a fixed encoding matrix where each position is assigned a unique combination of sine and cosine values across embedding dimensions. These encodings are added to the input token embeddings, enabling the model to utilize sequential order and relative positional information. The encodings are computed once for the maximum sequence length and reused during both training and inference.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "X01CLONoOAaI"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len=MAX_LEN, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            d_model (int): Embedding dimension.\n",
        "            max_seq_len (int): Maximum sequence length supported.\n",
        "            dropout (float): Dropout probability applied after adding encoding.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Initialize positional encoding matrix\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "\n",
        "        # Create position indices [0, 1, ..., max_seq_len - 1]\n",
        "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        # Compute the div_term for sine and cosine frequencies\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2).float()\n",
        "            * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        # Apply sine to even indices and cosine to odd indices\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Add batch dimension for broadcasting\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        # Register as buffer (not a learnable parameter)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Add positional encoding to input embeddings.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Shape (batch_size, seq_len, d_model)\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Position-aware embeddings with same shape as input.\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:, : x.size(1), :]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs-QOEq8lXfm"
      },
      "source": [
        "### 4.2 Transformer Encoder Classifier Architecture\n",
        "\n",
        "Then, we develop the Transformer-based text classifier architecture built on top of pretrained GloVe embeddings. It embeds each input token, adds positional encodings so the model can understand word order, and passes the sequence through a stack of Transformer encoder layers. Afterward it pools the encoded sequence into a single representation while ignoring padding, and feeds that vector into a linear classifier to produce logits for each class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0sViU8o6N9tL"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_class,\n",
        "        vocab_size,\n",
        "        freeze=True,\n",
        "        nhead=2,\n",
        "        dim_feedforward=128,\n",
        "        max_len=MAX_LEN,\n",
        "        num_layers=2,\n",
        "        dropout=0.1,\n",
        "        activation=\"gelu\",\n",
        "        classifier_dropout=0.1,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Transformer-based text classifier with pretrained GloVe embeddings.\n",
        "\n",
        "        Args:\n",
        "            num_class (int): Number of output classes.\n",
        "            vocab_size (int): Vocabulary size.\n",
        "            freeze (bool): Whether to freeze GloVe embeddings.\n",
        "            nhead (int): Number of attention heads.\n",
        "            dim_feedforward (int): Hidden dimension of feedforward layers.\n",
        "            max_len (int): Maximum supported sequence length.\n",
        "            num_layers (int): Number of Transformer encoder layers.\n",
        "            dropout (float): Dropout probability in encoder and positional encoding.\n",
        "            activation (str): Activation function for encoder layers.\n",
        "            classifier_dropout (float): Dropout before classifier (reserved).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding layer initialized from pretrained GloVe\n",
        "        self.emb = nn.Embedding.from_pretrained(\n",
        "            glove_embedding.vectors,\n",
        "            freeze=freeze,\n",
        "            padding_idx=PAD_IDX,\n",
        "        )\n",
        "\n",
        "        embedding_dim = self.emb.embedding_dim\n",
        "        self.d_model = embedding_dim\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoder = PositionalEncoding(\n",
        "            d_model=embedding_dim,\n",
        "            dropout=dropout,\n",
        "            max_seq_len=max_len,\n",
        "        )\n",
        "\n",
        "        # Transformer encoder stack\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embedding_dim,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "            activation=activation,\n",
        "        )\n",
        "\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_layers,\n",
        "        )\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Linear(\n",
        "            embedding_dim,\n",
        "            num_class\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            x (LongTensor): Token indices with shape (batch_size, seq_len)\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Logits with shape (batch_size, num_class)\n",
        "        \"\"\"\n",
        "\n",
        "        # Identify padding positions\n",
        "        padding_mask = (x == PAD_IDX)  # (batch_size, seq_len)\n",
        "\n",
        "        # Embed tokens and scale by sqrt(d_model)\n",
        "        x = self.emb(x) * math.sqrt(self.d_model)\n",
        "\n",
        "        # Add positional encoding\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        # Apply Transformer encoder with padding mask\n",
        "        x = self.transformer_encoder(\n",
        "            x,\n",
        "            src_key_padding_mask=padding_mask\n",
        "        )\n",
        "        # Masked mean pooling over sequence dimension\n",
        "        mask = (~padding_mask).unsqueeze(-1)  # (batch_size, seq_len, 1)\n",
        "        denom = mask.sum(dim=1).clamp(min=1)\n",
        "        x = (x * mask).sum(dim=1) / denom\n",
        "\n",
        "        # Final classification\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f49vazhleG_"
      },
      "source": [
        "Here we create an instance of Transformer classifier with two output classes and the vocabulary that matches the pretrained embeddings, then it is moved to the selected device so it can run on either the CPU or GPU depending on availability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "gPCltrYV_ofn"
      },
      "outputs": [],
      "source": [
        "# Instantiate the Transformer classifier\n",
        "classifier = TransformerEncoderClassifier(\n",
        "    num_class=2,          # Binary classification: Pro vs Con\n",
        "    vocab_size=vocab_size # Vocabulary size built from GloVe\n",
        ").to(device)              # Move model to the appropriate device (CPU/GPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CPkUeD-lgpc"
      },
      "source": [
        "### 4.3 Prediction and Evaluation functions\n",
        "\n",
        "For testing and evaluation, we define a predict and evaluation function. The predict function accepts a raw text review, tokenizes it, and converts tokens to vocabulary IDs. The function then runs the model in inference mode to obtain logits, selects the highest-scoring class, and returns the corresponding label (\"Pro\" or \"Con\"). Before training, the model is tested to confirm its predictions are effectively random."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SN2SFuaJBd3-",
        "outputId": "ba643968-8079-4541-8ed9-b590f34b8d8c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Pro'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "def predict(text, model):\n",
        "    \"\"\"\n",
        "    Predicts whether a review is a 'Pro' or 'Con'.\n",
        "\n",
        "    Process:\n",
        "    - Tokenize text and convert tokens to vocabulary indices.\n",
        "    - Add a batch dimension (model expects batches).\n",
        "    - Move tensor to the same device as the model.\n",
        "    - Return label.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        # Tokenize and get vocab embeddings\n",
        "        tokens = vocab(tokenizer(text))\n",
        "\n",
        "        # Convert to tensor and add batch dimension (1, seq_len)\n",
        "        text_tensor = torch.tensor(tokens).unsqueeze(0).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(text_tensor)\n",
        "\n",
        "        # Get predicted class index and map to label name\n",
        "        return labels[output.argmax(1).item()]\n",
        "\n",
        "# Check model prediction, not trained\n",
        "predict(\"This product is fantastic\", classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOrUQrjflzwd"
      },
      "source": [
        "The evaluation function evaluates the model on a held-out dataset by running it in evaluation mode, turning off gradient computation, and iterating through the dataloader to collect predictions. It compares each prediction with the true label, counts how many are correct, and finally returns the overall accuracy. We used Mixed precision is used to speed up evaluation without affecting results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qG-WLzWVGB9O"
      },
      "outputs": [],
      "source": [
        "def evaluate(dataloader, model_eval):\n",
        "    \"\"\"\n",
        "    Evaluates the model on test dataloader and returns accuracy.\n",
        "\n",
        "    Process:\n",
        "      - Switch model to eval mode (disables dropout, etc.)\n",
        "      - Disable gradients to speed things up and save memory\n",
        "      - Use mixed precision during forward passes\n",
        "      - Accumulate correct predictions and compute accuracy\n",
        "    \"\"\"\n",
        "    model_eval.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for label, text in dataloader:\n",
        "            # Move batch to device\n",
        "            label, text = label.to(device), text.to(device)\n",
        "\n",
        "            # Forward pass with Automatic Mixed Precision\n",
        "            with autocast():\n",
        "                output = model_eval(text)\n",
        "\n",
        "            # Handle models that return dicts (LoRA wrappers)\n",
        "            logits = output[\"logits\"] if isinstance(output, dict) else output\n",
        "\n",
        "            # Predicted class indices\n",
        "            predicted = torch.max(logits.data, 1)[1]\n",
        "\n",
        "            # Update metrics\n",
        "            total_acc += (predicted == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "\n",
        "    return total_acc / total_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuKW10rpl7VN"
      },
      "source": [
        "Now, we run the evaluation function on the test dataset using the current (untrained) classifier. Since the model hasn’t learned anything yet, the accuracy is expected to be close to random guessing, roughly around 50%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCtQd4ltGC-K",
        "outputId": "d60d735c-b79c-4a83-f6bc-542ea7f9fcf7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.49903008331743304"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Compute accuracy on test data, should be around 50%\n",
        "test_accuracy = evaluate(test_dataloader, classifier)\n",
        "test_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKJjskNW-SQR"
      },
      "source": [
        "## 5. Training Pipeline & Model Evaluation\n",
        "\n",
        "This section covers the complete training and evaluation workflow. We begin by configuring the model’s training setup with loss, optimizer, and scheduler. Next, we detail the customized training loop using mixed-precision for efficiency. Following training, the pipeline loads the previously trained model and performs inference, concluding with a final evaluation on the test set to report the model's generalization accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXZ9E9Cil_oC"
      },
      "source": [
        "### 5.1 AMP Customized Training Loop\n",
        "\n",
        "As a start, we define the main training loop. This function trains the model for a specified number of epochs, using mixed precision to speed up computations. In each epoch, it iterates through training batches to perform the forward pass, calculate loss, execute the backward pass, and update the model parameters. It also adjusts the learning rate with a scheduler. After processing all batches, it evaluates the model on the validation set and logs the epoch's training loss and validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "jiLF58uPaW1A"
      },
      "outputs": [],
      "source": [
        "def train_model(\n",
        "    model,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    scheduler,\n",
        "    train_dataloader,\n",
        "    valid_dataloader,\n",
        "    epochs=30\n",
        "):\n",
        "    \"\"\"\n",
        "    Train the model using mixed precision, track validation accuracy,\n",
        "    and keep the best accuracy seen so far.\n",
        "    \"\"\"\n",
        "\n",
        "    cum_loss_list = []   # Track total loss per epoch\n",
        "    acc_epoch = []       # Track validation accuracy per epoch\n",
        "    acc_old = 0          # Best validation accuracy\n",
        "    time_start = time.time()\n",
        "\n",
        "    # GradScaler supports Automatic Mixed Precision (AMP)\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for epoch in tqdm(range(1, epochs + 1)):\n",
        "        model.train()\n",
        "        cum_loss = 0\n",
        "\n",
        "        for idx, (label, text) in enumerate(train_dataloader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Move batch to GPU/CPU\n",
        "            label, text = label.to(device), text.to(device)\n",
        "\n",
        "            # Forward pass under AMP context\n",
        "            with autocast():\n",
        "                predicted_label = model(text)\n",
        "                loss = criterion(predicted_label, label)\n",
        "\n",
        "            # Backpropagation (scaled for numerical stability)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            cum_loss += loss.item()\n",
        "\n",
        "        # Learning-rate scheduling\n",
        "        scheduler.step()\n",
        "        print(f\"Epoch {epoch}/{epochs} - Loss: {cum_loss}\")\n",
        "\n",
        "        # Track metrics\n",
        "        cum_loss_list.append(cum_loss)\n",
        "\n",
        "        accu_val = evaluate(valid_dataloader, model)\n",
        "        acc_epoch.append(accu_val)\n",
        "\n",
        "        # Save best validation accuracy\n",
        "        if accu_val > acc_old:\n",
        "            print(accu_val)\n",
        "            acc_old = accu_val\n",
        "\n",
        "    time_end = time.time()\n",
        "    print(f\"Training time: {time_end - time_start:.2f} seconds\")\n",
        "\n",
        "    return model, cum_loss_list, acc_epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DujT3pMcmND6"
      },
      "source": [
        "### 5.2 Model Training Configuration\n",
        "\n",
        "Next, we define a helper function that either trains a new classifier or loads one that was previously saved. When training is enabled, it sets up the loss function, optimizer, and learning-rate scheduler, runs the training loop, and then saves the finished model to disk. When training is disabled, it simply loads the saved model file and makes it ready for use on the current device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "xLiRrUAfa3qx"
      },
      "outputs": [],
      "source": [
        "def train_classifier(\n",
        "    train=False,\n",
        "    n_epochs=50,\n",
        "    lr=0.01,\n",
        "    step_size=2.0,\n",
        "    gamma=0.8,\n",
        "    save_name=\"transformer_classifier\",\n",
        "    load_name=\"transformer_classifier\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Train the classifier from scratch OR load a previously saved model.\n",
        "\n",
        "    Args:\n",
        "        train (bool): If True, train and save a new model.\n",
        "                      If False, load an existing one.\n",
        "    \"\"\"\n",
        "\n",
        "    global vocab, classifier, PAD_IDX\n",
        "\n",
        "    if train:\n",
        "        # Training configuration\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.SGD(\n",
        "            classifier.parameters(),\n",
        "            lr=lr,\n",
        "            momentum=0.9\n",
        "        )\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "            optimizer,\n",
        "            step_size=step_size,\n",
        "            gamma=gamma\n",
        "        )\n",
        "        # Run training loop\n",
        "        train_model(\n",
        "            model=classifier,\n",
        "            optimizer=optimizer,\n",
        "            criterion=criterion,\n",
        "            scheduler=scheduler,\n",
        "            train_dataloader=train_dataloader,\n",
        "            valid_dataloader=valid_dataloader,\n",
        "            epochs=n_epochs,\n",
        "        )\n",
        "\n",
        "        # Save entire model object\n",
        "        torch.save(classifier, save_name + \".pth\")\n",
        "        print(\"Saved entire model\")\n",
        "\n",
        "    else:\n",
        "        # Load saved model\n",
        "        classifier = torch.load(\n",
        "            load_name + \".pth\",\n",
        "            map_location=device\n",
        "        )\n",
        "        print(\"Loaded entire model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMFPrksRmMc7"
      },
      "source": [
        "### 5.3 Model Inference & Performance Validation\n",
        "\n",
        "Due to GPU computation limits, the model was trained for 30 epochs. The final model object was saved and uploaded to Google Drive for storage. Now, for inference, we load that saved TransformerClassifier checkpoint from disk. After loading, we print its architecture to verify the layer structure and configuration match the original trained model, ensuring a correct recovery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "oW-oL3chEy9i",
        "outputId": "913d50cb-5919-46fa-9c18-d661f000cc9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Z2pimkOf28NFMmC3K866hOsW1yNVE6XL&confirm=t\n",
            "To: /content/transformer_classifier.pth\n",
            "100%|██████████| 162M/162M [00:02<00:00, 55.3MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'transformer_classifier.pth'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# Extract file id from the shareable link\n",
        "file_id = '1Z2pimkOf28NFMmC3K866hOsW1yNVE6XL'\n",
        "# Download using gdown\n",
        "url = f'https://drive.google.com/uc?id={file_id}&confirm=t'\n",
        "gdown.download(url, 'transformer_classifier.pth', quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viU3mBD8AsEt",
        "outputId": "d2528b76-726f-4ecc-c58e-2ff816668740"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded entire model\n",
            "Model Architecture:\n",
            " TransformerEncoderClassifier(\n",
            "  (emb): Embedding(400000, 100, padding_idx=1)\n",
            "  (pos_encoder): PositionalEncoding(\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (transformer_encoder): TransformerEncoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-1): 2 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
            "        )\n",
            "        (linear1): Linear(in_features=100, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): Linear(in_features=128, out_features=100, bias=True)\n",
            "        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (classifier): Linear(in_features=100, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "train_classifier(\n",
        "    train=False,\n",
        "    save_name=\"transformer_classifier\",\n",
        "    load_name=\"transformer_classifier\"\n",
        ")\n",
        "print(\"Model Architecture:\\n\", classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvL7GfC6mZ7D"
      },
      "source": [
        "Now, we run the `predict` function with the loaded model on the same example review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "64Cr9UY3y2QK",
        "outputId": "17e1fc32-ba17-4a2b-f528-de4119449dc4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Pro'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "predict(\"This product is fantastic\", classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGioVCbLmgcy"
      },
      "source": [
        "Finaly, we run the evaluate function on the trained classifier on the held-out test set and prints the overall accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDbx0FM_SYuS",
        "outputId": "bc661c00-7901-41bf-e184-10de2d1adde2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 94.54%\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the trained (or loaded) classifier on the test set\n",
        "test_accuracy = evaluate(test_dataloader, classifier)\n",
        "print(f\"Test accuracy: {test_accuracy:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28dhAMgzDzhK"
      },
      "source": [
        "## 6. Fine-Tunning Text Classifier with LoRA\n",
        "\n",
        "This section covers the parameter-efficient adaptation of the trained model using LoRA. We begin by creating a configuration wrapper to make the Transformer classifier compatible with PEFT tools. Next, we inject and configure the LoRA adapters into the model's attention and feed-forward layers. Finally, the process of loading the fine-tuned model and evaluating its improved performance on the test set is demonstrated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGpn7GYBmukv"
      },
      "source": [
        "### 6.1 PEFT Wrapper Configuration\n",
        "\n",
        "First, we create a configuration class that stores the key hyperparameters and metadata needed to describe the Transformer classifier in a way similar to Hugging Face models. It defines architectural details (like hidden size, number of layers, and attention heads), task settings such as number of labels and vocabulary size, label mappings, and a few utility fields used for compatibility and logging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "8htfmClUoBFE"
      },
      "outputs": [],
      "source": [
        "# Lightweight configuration object to mimic Hugging Face model configs\n",
        "class ModelConfig:\n",
        "    def __init__(self):\n",
        "        # Core architecture parameters\n",
        "        self.hidden_size = 100\n",
        "        self.num_hidden_layers = 2\n",
        "        self.num_attention_heads = 5\n",
        "        self.intermediate_size = 128\n",
        "\n",
        "        # Task / vocabulary settings\n",
        "        self.vocab_size = 400000\n",
        "        self.num_labels = 2\n",
        "        self.problem_type = \"single_label_classification\"\n",
        "\n",
        "        # Label mappings\n",
        "        self.id2label = {0: \"Con\", 1: \"Pro\"}\n",
        "        self.label2id = {\"Con\": 0, \"Pro\": 1}\n",
        "\n",
        "        # Misc\n",
        "        self.torch_dtype = \"float32\"\n",
        "        self._name_or_path = \"custom_model\"\n",
        "        self.use_return_dict = True\n",
        "        self.classifier_dropout = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiwBRIWGmwZX"
      },
      "source": [
        "Next, the wrapper class below helps us adapt the existing Transformer classifier so it can work with PEFT/LoRA. It stores the original trained model, exposes a configuration object that mimics the Hugging Face interface, and freezes all base parameters so that only LoRA adapters will be trained. Its forward method simply routes inputs to the underlying classifier while providing the standard signature expected by PEFT tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "FJy0DH0gBWj2"
      },
      "outputs": [],
      "source": [
        "# Wrapper to make our classifier compatible with PEFT / LoRA\n",
        "class TransformerEncoderClassifierWithLoRA(nn.Module):\n",
        "    def __init__(self, original_model):\n",
        "        \"\"\"\n",
        "        Wrap the trained classifier so LoRA can attach adapters.\n",
        "        The base model is frozen and only LoRA layers will be trained.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Keep reference to the original classifier\n",
        "        self.model = original_model\n",
        "\n",
        "        # Provide a config attribute (expected by PEFT / Transformers)\n",
        "        self.config = ModelConfig()\n",
        "\n",
        "        # Freeze all base model parameters\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):\n",
        "        \"\"\"\n",
        "        Standard forward signature\n",
        "        \"\"\"\n",
        "        logits = self.model(input_ids)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Mks8In1m5Au"
      },
      "source": [
        "### 6.2 Adapter Injection in Attention/Linear Layers\n",
        "\n",
        "Here the frozen Transformer classifier is wrapped so LoRA can attach lightweight adapters, and a configuration is defined describing how those adapters should behave. Below we define the LoRA settings such as where adapters are inserted, how large they are, how strongly their updates are scaled, and which parts of the base model are still allowed to train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "PueLzKE2g6Ns"
      },
      "outputs": [],
      "source": [
        "# Wrap the frozen classifier so LoRA can attach adapters\n",
        "model_with_lora = TransformerEncoderClassifierWithLoRA(classifier)\n",
        "\n",
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=8,                               # Rank matrices\n",
        "    lora_alpha=16,                     # Scaling factor for LoRA updates\n",
        "    target_modules=[\"linear1\", \"linear2\", \"out_proj\"],  # Layers to inject adapters\n",
        "    lora_dropout=0.1,                  # Dropout\n",
        "    bias=\"all\",                        # Also train biases\n",
        "    task_type=\"SEQ_CLS\",               # Sequence classification task\n",
        "    modules_to_save=[\"classifier\"],    # Keep final binary classifier trainable\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg3CQMRqnHyX"
      },
      "source": [
        "This step applies the LoRA configuration to the wrapped model, injecting trainable adapter layers while keeping the original Transformer mostly frozen. After attaching the adapters, we print out which parameters will actually be updated, confirming that only the LoRA components will be trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srg4_5wcCjl1",
        "outputId": "7ef57d9c-2a78-4f51-fd16-b929b62de931"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 12,354 || all params: 40,144,156 || trainable%: 0.0308\n"
          ]
        }
      ],
      "source": [
        "# Attach LoRA adapters to the model\n",
        "classifier = get_peft_model(model_with_lora, lora_config)\n",
        "\n",
        "# Display which parameters will update during training\n",
        "classifier.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtqdle6tnQC0"
      },
      "source": [
        "### 6.3 Parameter-Efficient Training & Evaluation\n",
        "\n",
        "We trained the LoRA classifier for 20 epochs, to avoid retraining it is saved on Drive and loaded here. By printing the model architecture confirms that LoRA adapters were successfully injected into the Transformer layers, keeping the base model frozen while the classifier head remains trainable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "1JVPhpFrFFMM",
        "outputId": "0d61ed27-4c7c-4171-e8bd-7d46b1a370e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1YozUCXDvaLhaWQw-lRHuILP-oAwv_2ph&confirm=t\n",
            "To: /content/transformer_classifier_lora.pth\n",
            "100%|██████████| 162M/162M [00:03<00:00, 40.7MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'transformer_classifier_lora.pth'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# Extract FILE_ID from the shareable link\n",
        "file_id = '1YozUCXDvaLhaWQw-lRHuILP-oAwv_2ph'\n",
        "\n",
        "# Download using gdown\n",
        "url = f'https://drive.google.com/uc?id={file_id}&confirm=t'\n",
        "gdown.download(url, 'transformer_classifier_lora.pth', quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXQQuqEggl_v",
        "outputId": "ab3a17e1-fdac-4243-e6fb-51616dac8022"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded entire model\n",
            "Classifier architecure with LoRA:  PeftModelForSequenceClassification(\n",
            "  (base_model): LoraModel(\n",
            "    (model): TransformerEncoderClassifierWithLoRA(\n",
            "      (model): TransformerEncoderClassifier(\n",
            "        (emb): Embedding(400000, 100, padding_idx=1)\n",
            "        (pos_encoder): PositionalEncoding(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (transformer_encoder): TransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-1): 2 x TransformerEncoderLayer(\n",
            "              (self_attn): MultiheadAttention(\n",
            "                (out_proj): lora.Linear(\n",
            "                  (base_layer): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=100, out_features=8, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=8, out_features=100, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                )\n",
            "              )\n",
            "              (linear1): lora.Linear(\n",
            "                (base_layer): Linear(in_features=100, out_features=128, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=100, out_features=8, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=8, out_features=128, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (linear2): lora.Linear(\n",
            "                (base_layer): Linear(in_features=128, out_features=100, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=128, out_features=8, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=8, out_features=100, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (classifier): ModulesToSaveWrapper(\n",
            "          (original_module): Linear(in_features=100, out_features=2, bias=True)\n",
            "          (modules_to_save): ModuleDict(\n",
            "            (default): Linear(in_features=100, out_features=2, bias=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Train = False - loads already fine-tuned LoRA model\n",
        "train_classifier(\n",
        "    train=False,\n",
        "    n_epochs=40,\n",
        "    lr=0.01,\n",
        "    step_size=5.0,\n",
        "    gamma=0.8,\n",
        "    save_name=\"transformer_classifier_lora\",\n",
        "    load_name=\"transformer_classifier_lora\"\n",
        "    )\n",
        "\n",
        "print(\"Classifier architecure with LoRA: \", classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lH9c1LMnZ4Q"
      },
      "source": [
        "Finally, the model with trained LoRA adapters is evaluated on the test set, achieving a slight improvement in accuracy over the baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZtKM2IAkXp-",
        "outputId": "a3db99bb-c3fc-4893-c178-8cf4ae0eb22d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 94.60%\n"
          ]
        }
      ],
      "source": [
        "# Measure final generalization performance\n",
        "test_accuracy = evaluate(test_dataloader, classifier)\n",
        "print(f\"Test accuracy: {test_accuracy:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYWu-MRGQcSi"
      },
      "source": [
        "## 7. Summarization System Implementation\n",
        "\n",
        "In this section we build the final integrated system for review analysis. First, we set up a Hugging Face summarization pipeline. Next, we implement a text cleaning and sentence splitting function. Then, we create a combined pipeline that extracts individual sentences, classifies each one as a Pro or a Con, and generates separate summaries for each category. Finally, we test the complete system on a set of real Amazon reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3mMzgvvHvM1"
      },
      "source": [
        "### 7.1 Summarization Pipeline\n",
        "\n",
        "We begin by creating a Hugging Face pipeline for text summarization using the pretrained `philschmid/bart-large-cnn-samsum` model. This pipeline automatically handles tokenization, model inference, and decoding, allowing us to summarize input text with a single function call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "BdAPzbJt6CRC"
      },
      "outputs": [],
      "source": [
        "# Hugging Face pipeline for text summarization\n",
        "summarizer = pipeline(\n",
        "    task=\"summarization\",\n",
        "    model=\"philschmid/bart-large-cnn-samsum\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imHPBZWStYFi"
      },
      "source": [
        "### 7.2 Text Cleaning & Sentence Processing\n",
        "\n",
        "To handle summarization, first we implement a text cleaning pipeline that removes numbers, special characters, and pronouns while preserving letters, spaces, and basic punctuation (periods, exclamation points, question marks). The pipeline also normalizes whitespace by collapsing multiple spaces into single spaces and trimming leading/trailing whitespace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "-RokeFPQx3v6"
      },
      "outputs": [],
      "source": [
        "def clean_text_basic(text):\n",
        "    \"\"\"\n",
        "    Basic cleaning: remove numbers and special symbols, keep letters, spaces, and basic punctuation\n",
        "    \"\"\"\n",
        "    # Remove numbers and leading/trailing whitespace\n",
        "    text = re.sub(r'\\d+', '', text.strip())\n",
        "\n",
        "    # Remove special symbols but keep basic punctuation (.!?,)\n",
        "    text = re.sub(r'[^\\w\\s.!?]', '', text)\n",
        "    text = text.replace('\\n', '. ')\n",
        "\n",
        "    # Replace any non-space character after . or , with a space\n",
        "    text = re.sub(r'([.,])\\S', r'\\1 ', text)\n",
        "\n",
        "    pronouns = [\n",
        "    # Personal pronouns\n",
        "    'I', 'me', 'my', 'mine', 'myself',\n",
        "    'you', 'your', 'yours', 'yourself', 'yourselves',\n",
        "    'he', 'him', 'his', 'himself',\n",
        "    'she', 'her', 'hers', 'herself',\n",
        "    'it', 'its', 'itself',\n",
        "    'we', 'us', 'our', 'ours', 'ourselves',\n",
        "    'they', 'them', 'their', 'theirs', 'themselves'\n",
        "    ]\n",
        "\n",
        "    # Remove pronouns and related\n",
        "    pattern = r'\\b(' + '|'.join(map(re.escape, pronouns)) + r')\\b'\n",
        "    text = re.sub(pattern, \"\", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\bI'?ve\\b\", \"\", text)\n",
        "    text = re.sub(r\"\\bI'?m\\b\", \"\", text)\n",
        "    text = re.sub(r\"\\bI'?ll\\b\", \"\", text)\n",
        "    text = re.sub(r\"\\bI'?d\\b\", \"\", text)\n",
        "    text = re.sub(r\"\\bam\\b\", \"\", text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXnoGNEeteO6"
      },
      "source": [
        "### 7.3 Integrated Pros/Cons Extraction Pipeline\n",
        "\n",
        "Finally, we implement the summarizer and classifier, the function takes a piece of text, splits it into sentences, classifies each sentence as a *Pro* or *Con* using the trained classifier, and then summarizes the grouped Pro and Con sentences separately using the Hugging Face summarization pipeline. It returns tuple of a structured strings showing summarized Pros and Cons, skipping empty sections if no sentences fall into one category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "SmsbN8EO_Y-w"
      },
      "outputs": [],
      "source": [
        "def ProsConsSummarizer(text, summarizer=summarizer):\n",
        "    \"\"\"\n",
        "    Split text into sentences, classify each sentence as Pro/Con,\n",
        "    summarize each sentence, and return grouped Pros/Cons text.\n",
        "    \"\"\"\n",
        "    # Clean and tokenize textual input\n",
        "    sentences = nltk.sent_tokenize(clean_text_basic(text))\n",
        "\n",
        "    pros = \"\"\n",
        "    cons = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Predict sentiment category using our classifier\n",
        "        prediction = predict(sentence, classifier)\n",
        "\n",
        "        sentence = sentence.capitalize()\n",
        "\n",
        "        if prediction == \"Pro\":\n",
        "            pros += sentence + \", \"\n",
        "        else:\n",
        "            cons += sentence + \", \"\n",
        "\n",
        "    if pros == \"\" and cons == \"\":\n",
        "      pros = \"No Pros found in review.\"\n",
        "      cons = \"No Cons found in review.\"\n",
        "    elif pros == \"\":\n",
        "      pros = \"No Pros found in review.\"\n",
        "      cons = \"- \" + summarizer(cons, max_length=100, min_length=50)[0][\"summary_text\"].strip()\n",
        "      cons = re.sub(r'\\.\\s+', '.\\n- ', cons)\n",
        "    elif cons == \"\":\n",
        "      cons = \"No Cons found in review.\"\n",
        "      pros = \"- \" + summarizer(pros, max_length=100, min_length=50)[0][\"summary_text\"].strip()\n",
        "      pros = re.sub(r'\\.\\s+', '.\\n- ', pros)\n",
        "    else:\n",
        "      pros = \"- \" + summarizer(pros, max_length=100, min_length=25)[0][\"summary_text\"].strip()\n",
        "      pros = re.sub(r'\\.\\s+', '.\\n- ', pros)\n",
        "      cons = \"- \" + summarizer(cons, max_length=100, min_length=25)[0][\"summary_text\"].strip()\n",
        "      cons = re.sub(r'\\.\\s+', '.\\n- ', cons)\n",
        "    return pros, cons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_hbkjXwtgQQ"
      },
      "source": [
        "### 7.4 Example Reviews for Testing\n",
        "These five reviews are real Amazon product reviews expressing both positive and negative aspects. They include detailed feedback about fit, usability, performance, comfort, and design issues, providing a mix of pros and cons that can be analyzed or summarized by the classifier and summarization pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "zPPoxjvcXXA1"
      },
      "outputs": [],
      "source": [
        "review1 = \"\"\"\n",
        "I absolutely loved the toe of this shoe, they seem to be exceptionally comfortable,lightweight and fit nicely. However the issue that I had with these shoes is the arch, my Arch just rubbed on it, too hard and way to prominent..if they could figure out how to redesign the arch in the shoe it would be a fabulous walking shoe.\n",
        "\"\"\"\n",
        "\n",
        "review2 = \"\"\"\n",
        "I ordered these sneakers in a size 7 wide, even though I usually wear a 6 or 6.5, because I wanted a little extra room. For me personally, that sizing choice worked fairly well. They felt slightly loose in the heel area but still comfortable overall.\n",
        "The shoes came in black with some dark gray tones and a white logo on the side. The style looked clean and simple to me, and I thought they seemed stylish. The bottom soles seemed to have good traction, and when I tested them on tile and hardwood, they didn’t seem slippery. The shoes also felt very lightweight, which I liked since I often have trouble finding comfortable sneakers that don’t seem to weigh down my feet.\n",
        "The fabric on the upper part of the shoes seemed breathable and flexible, with what seemed like mesh like areas. I think that’s a nice touch for comfort and keeping feet cooler. The shoes can be tied, but they also seem to work well as slip ons, and there’s a small tab or loop on the back that makes them easier to pull on and off.\n",
        "Inside, the lining felt soft, but I did notice that the insole didn't seem to be secured, it lifted slightly. At first, I thought that might be an issue, but after wearing them for a bit, it didn’t seem to move around when I walked. Still, I think it’s worth mentioning in case others prefer shoes with the insole sewn in. The cushioning itself felt a bit thin but they were still comfortable to me overall.\n",
        "The overall construction seemed decent, and the shoes felt breathable, light, and seemed easy to wear. For me personally, the comfort level was decent overall. I liked how flexible they felt and that the traction worked well across different surfaces.\n",
        "Overall, these sneakers seemed lightweight, breathable, and seemed stylish. For me personally, they met most of what I was hoping for, and I plan to keep wearing them. The only small downsides for me were the slightly loose fit at the heel for me and the insole not being fully secured, but they still felt comfortable overall. I’ll continue wearing them and may update my review later if anything changes with more wear.\n",
        "\"\"\"\n",
        "\n",
        "review3 = \"\"\"\n",
        "I’ve had this machine now for a month and I absolutely LOVE IT!!\n",
        "-pulls good shots\n",
        "-easy to use and clean\n",
        "The only bad thing i’ve found about it so far, it’s a little noisy but it’s not terrible! (most machines are going to be noisy)\n",
        "The frothing wand is really good if you know how to use it, I recommend watching videos on how to properly steam milk if you don’t know how!\n",
        "\"\"\"\n",
        "\n",
        "review4 = \"\"\"\n",
        "I love the oversized hood and the jacket fits well though if you're big chested on a small frame as I am, I'd recommend going a size larger than you're used to wearing. I'm a med normally in coats, but this one doesn't give much room across my chest. Also, I was unhappy about the inner pockets because they were not stitched at the bottom,\n",
        "so although it looked like a pocket, it was clearly not one. I stitched the bottoms of the pockets closed because I love inside pockets. I have to say the value for the money spent is very good though.\n",
        "\"\"\"\n",
        "\n",
        "review5 = \"\"\"\n",
        "Listen. These shoes fit great, they look sporty, and they're perfect for my wide duck feet (don’t judge me). I bought the extra wide size and my toes are living their best life now.\n",
        "But here’s the twist. Getting them on is like wrestling an octopus into a sock. The opening is a stiff little circle with all the give of a jealous ex. There’s no stretch. None. Zip. You will earn your right to wear these shoes through sheer determination, upper body strength, and possibly a shoehorn named Excalibur.\n",
        "Once you're in though? Cloud city. My feet are happy, comfy, and supported. Totally worth the five minute wrestling match. Great price too. Just warm up first. Maybe stretch. Hydrate.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7XUlc8hLnvR"
      },
      "source": [
        "Next we try our integrated pros and cons extraction pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "E5mysj7mLmpU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8834e426-5de0-4be4-d16c-318ff4fdaa9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pros:\n",
            " - The frothing wand is good if you know how to use it.\n",
            "- The machine is easy to use and clean. \n",
            "Cons:\n",
            " - The only bad thing so far is that the machines are noisy, but it's not terrible as most machines are going to be noisy.\n"
          ]
        }
      ],
      "source": [
        "pros, cons = ProsConsSummarizer(review3, summarizer=summarizer)\n",
        "print(f\"Pros:\\n {pros} \\nCons:\\n {cons}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3VKcHMZt7ED"
      },
      "source": [
        "## 8. Gradio Interface Implementation for User Interaction\n",
        "\n",
        "As a last step, we build an interactive web app that analyzes product reviews to automatically identify pros and cons. It creates a clean interface where one can paste any review text, and it will extract and display the positive and negative points in separate, color-coded sections, complete with helpful statistics and example reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "gWEdKGnEBbZx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "52bd940b-711e-4790-e3a6-ab8eb9166dac"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7860, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Create the Gradio interface\n",
        "with gr.Blocks(\n",
        "    title=\"Review Pros & Cons Summarizer\",\n",
        "    theme=gr.themes.Soft(),\n",
        "    css=\"\"\"\n",
        "        .pros-card {\n",
        "            border: 1px solid #10b981;\n",
        "            border-radius: 10px;\n",
        "            padding: 15px;\n",
        "            background: linear-gradient(135deg, #f0fdf4 0%, #dcfce7 100%);\n",
        "        }\n",
        "        .cons-card {\n",
        "            border: 1px solid #ef4444;\n",
        "            border-radius: 10px;\n",
        "            padding: 15px;\n",
        "            background: linear-gradient(135deg, #fef2f2 0%, #fee2e2 100%);\n",
        "        }\n",
        "        .pros-header {\n",
        "            color: #059669;\n",
        "            font-weight: bold;\n",
        "            margin-bottom: 10px;\n",
        "        }\n",
        "        .cons-header {\n",
        "            color: #dc2626;\n",
        "            font-weight: bold;\n",
        "            margin-bottom: 10px;\n",
        "        }\n",
        "        .stats {\n",
        "            background: #f8fafc;\n",
        "            padding: 10px;\n",
        "            border-radius: 8px;\n",
        "            margin-top: 10px;\n",
        "            font-size: 0.9em;\n",
        "            border-left: 3px solid #94a3b8;\n",
        "        }\n",
        "        .container {\n",
        "            max-width: 900px;\n",
        "            margin: auto;\n",
        "        }\n",
        "        .output-text {\n",
        "            font-size: 14px;\n",
        "            line-height: 1.6;\n",
        "        }\n",
        "        .btn-primary {\n",
        "            background: linear-gradient(135deg, #3b82f6 0%, #1d4ed8 100%);\n",
        "            border: none;\n",
        "        }\n",
        "        .btn-secondary {\n",
        "            background: white;\n",
        "        }\n",
        "    \"\"\"\n",
        ") as demo:\n",
        "\n",
        "    # Header\n",
        "    gr.Markdown(\"\"\"\n",
        "    # 🧩 Review Pros & Cons Summarizer\n",
        "    **Automatically extract and summarize positive and negative aspects from reviews**\n",
        "\n",
        "    *Enter any review text below to see the extracted pros and cons in separate sections.*\n",
        "    \"\"\", elem_classes=\"header\")\n",
        "\n",
        "    # Main layout with two columns\n",
        "    with gr.Row():\n",
        "        # Left column - Input\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### 📝 **Input Review**\")\n",
        "\n",
        "            input_text = gr.Textbox(\n",
        "                label=\"\",\n",
        "                lines=10,\n",
        "                max_lines=30,\n",
        "                placeholder='''Paste your product review, customer feedback, or any opinion text here...\\n\\nExample: \"The camera quality is amazing but battery life could be better.\"''',\n",
        "                show_label=False,\n",
        "            )\n",
        "\n",
        "            with gr.Row():\n",
        "                analyze_btn = gr.Button(\n",
        "                    \"🔍 Analyze Review\",\n",
        "                    variant=\"primary\",\n",
        "                    scale=2,\n",
        "                    elem_classes=\"btn-primary\"\n",
        "                )\n",
        "                clear_btn = gr.Button(\n",
        "                    \"🗑️ Clear\",\n",
        "                    variant=\"secondary\",\n",
        "                    scale=1,\n",
        "                    elem_classes=\"btn-secondary\"\n",
        "                )\n",
        "\n",
        "            # Example section\n",
        "            gr.Markdown(\"### 💡 **Quick Examples**\")\n",
        "            gr.Examples(\n",
        "                examples=[[review1], [review2], [review3], [review4], [review5]],\n",
        "                inputs=input_text,\n",
        "                label=\"Click any example to load it\",\n",
        "                examples_per_page=3\n",
        "            )\n",
        "\n",
        "        # Right column - Output\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### 📊 **Analysis Results**\")\n",
        "\n",
        "            # Pros section\n",
        "            with gr.Column(elem_classes=\"pros-card\"):\n",
        "                gr.Markdown(\"##### ✅ **PROS**\", elem_classes=\"pros-header\")\n",
        "                pros_output = gr.Textbox(\n",
        "                    label=\"\",\n",
        "                    lines=3,\n",
        "                    max_lines=10,\n",
        "                    interactive=False,\n",
        "                    show_label=False,\n",
        "                    elem_classes=\"output-text\",\n",
        "                    placeholder=\"Positive aspects will appear here...\",\n",
        "                    show_copy_button=True\n",
        "                )\n",
        "\n",
        "                # Optional stats for pros\n",
        "                with gr.Accordion(\"📈 Details\", open=False):\n",
        "                    pros_stats = gr.Markdown(\"\")\n",
        "\n",
        "            # Cons section\n",
        "            with gr.Column(elem_classes=\"cons-card\"):\n",
        "                gr.Markdown(\"##### ❌ **CONS**\", elem_classes=\"cons-header\")\n",
        "                cons_output = gr.Textbox(\n",
        "                    label=\"\",\n",
        "                    lines=3,\n",
        "                    max_lines=10,\n",
        "                    interactive=False,\n",
        "                    show_label=False,\n",
        "                    elem_classes=\"output-text\",\n",
        "                    placeholder=\"Negative aspects will appear here...\",\n",
        "                    show_copy_button=True\n",
        "                )\n",
        "\n",
        "                # Optional stats for cons\n",
        "                with gr.Accordion(\"📈 Details\", open=False):\n",
        "                    cons_stats = gr.Markdown(\"\")\n",
        "\n",
        "    # Footer\n",
        "    gr.Markdown(\"\"\"\n",
        "    ---\n",
        "    *Note: The analysis splits text into sentences, classifies each as Pro or Con, and provides concise summaries.*\n",
        "    \"\"\")\n",
        "\n",
        "    # Helper function to calculate statistics\n",
        "    def calculate_stats(pros_text, cons_text):\n",
        "        \"\"\"Calculate statistics for display\"\"\"\n",
        "        pros_words = len(pros_text.split()) if pros_text!=\"No Pros found in review.\" else 0\n",
        "        cons_words = len(cons_text.split()) if cons_text!=\"No Cons found in review.\" else 0\n",
        "\n",
        "        pros_stats_text = f\"\"\"\n",
        "        **Pros Statistics:**\\n\n",
        "        • Words: {pros_words}\n",
        "        • Characters: {len(pros_text) if pros_text else 0}\n",
        "        • Has content: {\"✅ Yes\" if pros_text!= \"No Pros found in review.\" else \"❌ No\"}\n",
        "        \"\"\"\n",
        "\n",
        "        cons_stats_text = f\"\"\"\n",
        "        **Cons Statistics:**\\n\n",
        "        • Words: {cons_words}\n",
        "        • Characters: {len(cons_text) if cons_text else 0}\n",
        "        • Has content: {\"✅ Yes\" if cons_text!= \"No Cons found in review.\" else \"❌ No\"}\n",
        "        \"\"\"\n",
        "\n",
        "        return pros_stats_text, cons_stats_text\n",
        "\n",
        "    # Main processing function\n",
        "    def process_text(text):\n",
        "        \"\"\"Process the text and return all outputs\"\"\"\n",
        "        # Call your function directly - it returns (pros, cons)\n",
        "        pros, cons = ProsConsSummarizer(text)\n",
        "\n",
        "        # Calculate statistics\n",
        "        pros_stats_text, cons_stats_text = calculate_stats(pros, cons)\n",
        "\n",
        "        # Return all outputs in order\n",
        "        return pros, cons, pros_stats_text, cons_stats_text\n",
        "\n",
        "    # Connect the analyze button\n",
        "    analyze_btn.click(\n",
        "        fn=process_text,\n",
        "        inputs=input_text,\n",
        "        outputs=[\n",
        "            pros_output,     # Main pros display\n",
        "            cons_output,     # Main cons display\n",
        "            pros_stats,      # Pros statistics\n",
        "            cons_stats      # Cons statistics\n",
        "            ]\n",
        "    )\n",
        "\n",
        "    # Clear button functionality\n",
        "    def clear_all():\n",
        "        \"\"\"Clear all inputs and outputs\"\"\"\n",
        "        return [\"\", \"\", \"\", \"\", \"\"]\n",
        "\n",
        "    clear_btn.click(\n",
        "        fn=clear_all,\n",
        "        inputs=None,\n",
        "        outputs=[\n",
        "            input_text,      # Input textbox\n",
        "            pros_output,     # Pros output\n",
        "            cons_output,     # Cons output\n",
        "            pros_stats,      # Pros stats\n",
        "            cons_stats      # Cons stats\n",
        "            ]\n",
        "    )\n",
        "\n",
        "    # Auto-clear when input changes\n",
        "    def clear_on_input_change(text):\n",
        "         return [\"\", \"\", \"\", \"\", \"\"]\n",
        "\n",
        "    input_text.change(\n",
        "         fn=clear_on_input_change,\n",
        "         inputs=input_text,\n",
        "         outputs=[pros_output, cons_output, pros_stats, cons_stats]\n",
        "    )\n",
        "\n",
        "# Launch the application\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(\n",
        "        server_name=\"0.0.0.0\" if os.getenv(\"SPACE_ID\") else \"127.0.0.1\",\n",
        "        share=False,\n",
        "        show_error=True,\n",
        "        favicon_path=None,\n",
        "        auth=None,\n",
        "        auth_message=None,\n",
        "        prevent_thread_lock=False,\n",
        "        show_api=False,\n",
        "        debug=False,\n",
        "        quiet=True\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}